# Public Benchmark and Leaderboard

This benchmark is credibility-first: scores are generated by calling live detector endpoints, not by replaying fixture `baseline_score` fields.

## Tasks
1. AI-generated vs human detection (multi-domain)
2. Source attribution (heuristic baseline over model families)
3. Tamper robustness (paraphrase, translation, human edits)
4. Audio AI-vs-human detection (**experimental**)
5. Video AI-vs-human detection (**experimental**)

## Dataset contract
Core fields used by live benchmark datasets:
- `sample_id`
- `task`
- `domain`
- `label_is_ai`
- `modality`
- `input_ref` (repo-relative path or URL)

Optional fields:
- `transform` (tamper task)
- `source_model_family` (attribution task)
- `predicted_model_family_baseline` (heuristic attribution baseline)

## Run locally (single command)
`make benchmark-public` now auto-starts a local backend (`127.0.0.1:8000`) when needed:

```bash
make benchmark-public
```

Useful overrides:

```bash
# Use an already-running remote backend
BACKEND_URL=https://your-api.example.com AUTO_START_BACKEND=false make benchmark-public

# Skip regression gate for quick smoke checks
SKIP_REGRESSION_CHECK=true make benchmark-public
```

Equivalent explicit command:

```bash
python benchmark/eval/run_public_benchmark.py \
  --datasets-dir benchmark/datasets \
  --output-dir benchmark/results/latest \
  --leaderboard-output benchmark/leaderboard/leaderboard.json \
  --model-id baseline-heuristic-v1.0-live \
  --decision-threshold 0.45 \
  --backend-url http://127.0.0.1:8000 \
  --live-mode true
```

## Outputs
- `benchmark/results/latest/benchmark_results.json`
- `benchmark/results/latest/scored_samples.jsonl`
- `benchmark/results/latest/baseline_results.md`
- `benchmark/results/latest/regression_check.json`
- `benchmark/results/latest/regression_check.md`
- `benchmark/leaderboard/leaderboard.json`

Open `benchmark/leaderboard/index.html` to view leaderboard results.

## Publish as public page
- Workflow: `.github/workflows/publish-leaderboard.yml`
- It rebuilds benchmark artifacts, checks regression gates, and publishes `benchmark/leaderboard` to GitHub Pages.
- `scored_samples.jsonl` is uploaded as a build artifact for reproducibility.
