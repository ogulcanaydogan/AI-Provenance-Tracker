name: Publish Benchmark Leaderboard

on:
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - "benchmark/**"
      - ".github/workflows/publish-leaderboard.yml"

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install backend dependencies
        run: |
          cd backend
          pip install -e ".[dev]"
      - name: Start backend for live benchmark
        env:
          RUN_SCHEDULER_IN_API: "false"
          CONSENSUS_ENABLED: "false"
          C2PA_ENABLED: "false"
          RATE_LIMIT_REQUESTS: "10000"
          RATE_LIMIT_MEDIA_REQUESTS: "10000"
          RATE_LIMIT_BATCH_REQUESTS: "10000"
        run: |
          mkdir -p benchmark/results/latest
          cd backend
          nohup uvicorn app.main:app --host 127.0.0.1 --port 8000 > ../benchmark/results/latest/backend.log 2>&1 &
          echo $! > ../benchmark/results/latest/backend.pid
          for i in {1..45}; do
            if curl -fsS http://127.0.0.1:8000/health > /dev/null; then
              break
            fi
            sleep 1
          done
          curl -fsS http://127.0.0.1:8000/health > /dev/null || {
            echo "Backend failed to start"
            cat ../benchmark/results/latest/backend.log
            exit 1
          }
      - name: Rebuild leaderboard artifacts
        run: |
          python benchmark/eval/run_public_benchmark.py \
            --datasets-dir benchmark/datasets \
            --output-dir benchmark/results/latest \
            --leaderboard-output benchmark/leaderboard/leaderboard.json \
            --model-id baseline-heuristic-pages \
            --decision-threshold 0.45 \
            --backend-url http://127.0.0.1:8000 \
            --live-mode true
      - name: Check benchmark regressions
        run: |
          python benchmark/eval/check_benchmark_regression.py \
            --current benchmark/results/latest/benchmark_results.json \
            --baseline benchmark/baselines/public_benchmark_snapshot.json \
            --report-json benchmark/results/latest/regression_check.json \
            --report-md benchmark/results/latest/regression_check.md
      - name: Stop backend
        if: always()
        run: |
          if [ -f benchmark/results/latest/backend.pid ]; then
            kill "$(cat benchmark/results/latest/backend.pid)" || true
          fi
      - name: Upload benchmark run artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-run-artifacts
          path: |
            benchmark/results/latest/benchmark_results.json
            benchmark/results/latest/scored_samples.jsonl
            benchmark/results/latest/baseline_results.md
            benchmark/results/latest/regression_check.json
            benchmark/results/latest/regression_check.md
          retention-days: 14
      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: benchmark/leaderboard
      - name: Deploy disabled notice
        if: ${{ vars.ENABLE_GH_PAGES_DEPLOY != 'true' }}
        run: echo "ENABLE_GH_PAGES_DEPLOY is not true; skipping deploy job."

  deploy:
    if: ${{ vars.ENABLE_GH_PAGES_DEPLOY == 'true' }}
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
