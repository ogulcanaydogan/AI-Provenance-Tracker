name: Public Provenance Benchmark

on:
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - "benchmark/**"
      - ".github/workflows/public-benchmark.yml"
  pull_request:
    paths:
      - "benchmark/**"
      - ".github/workflows/public-benchmark.yml"

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install backend dependencies
        run: |
          cd backend
          pip install -e ".[dev]"
      - name: Start backend for live benchmark
        env:
          RUN_SCHEDULER_IN_API: "false"
          CONSENSUS_ENABLED: "false"
          C2PA_ENABLED: "false"
          RATE_LIMIT_REQUESTS: "10000"
          RATE_LIMIT_MEDIA_REQUESTS: "10000"
          RATE_LIMIT_BATCH_REQUESTS: "10000"
        run: |
          mkdir -p benchmark/results/ci
          cd backend
          nohup uvicorn app.main:app --host 127.0.0.1 --port 8000 > ../benchmark/results/ci/backend.log 2>&1 &
          echo $! > ../benchmark/results/ci/backend.pid
          for i in {1..45}; do
            if curl -fsS http://127.0.0.1:8000/health > /dev/null; then
              break
            fi
            sleep 1
          done
          curl -fsS http://127.0.0.1:8000/health > /dev/null || {
            echo "Backend failed to start"
            cat ../benchmark/results/ci/backend.log
            exit 1
          }
      - name: Run public benchmark
        run: |
          python benchmark/eval/run_public_benchmark.py \
            --datasets-dir benchmark/datasets \
            --output-dir benchmark/results/ci \
            --leaderboard-output benchmark/results/ci/leaderboard.json \
            --model-id baseline-heuristic-ci \
            --decision-threshold 0.45 \
            --backend-url http://127.0.0.1:8000 \
            --live-mode true
      - name: Check benchmark regressions
        run: |
          python benchmark/eval/check_benchmark_regression.py \
            --current benchmark/results/ci/benchmark_results.json \
            --baseline benchmark/baselines/public_benchmark_snapshot.json \
            --report-json benchmark/results/ci/regression_check.json \
            --report-md benchmark/results/ci/regression_check.md
      - name: Stop backend
        if: always()
        run: |
          if [ -f benchmark/results/ci/backend.pid ]; then
            kill "$(cat benchmark/results/ci/backend.pid)" || true
          fi
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v6
        with:
          name: public-benchmark-artifacts
          path: benchmark/results/ci
          retention-days: 14
